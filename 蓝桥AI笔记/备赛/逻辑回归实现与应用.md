# é€»è¾‘å›žå½’

é€»è¾‘å›žå½’ï¼ˆLogistic Regressionï¼‰ï¼Œåˆå«é€»è¾‘æ–¯è’‚å›žå½’ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ç§ååˆ†åŸºç¡€çš„åˆ†ç±»æ–¹æ³•ï¼Œç”±äºŽç®—æ³•ç®€å•è€Œé«˜æ•ˆï¼Œåœ¨å®žé™…åœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚æœ¬æ¬¡å®žéªŒä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢ç´¢é€»è¾‘å›žå½’çš„åŽŸç†åŠç®—æ³•å®žçŽ°ï¼Œå¹¶ä½¿ç”¨ scikit-learn æž„å»ºé€»è¾‘å›žå½’åˆ†ç±»é¢„æµ‹æ¨¡åž‹ã€‚
#### çŸ¥è¯†ç‚¹

-   çº¿æ€§å¯åˆ†å’Œä¸å¯åˆ†
-   Sigmoid åˆ†å¸ƒå‡½æ•°
-   é€»è¾‘å›žå½’æ¨¡åž‹
-   å¯¹æ•°æŸå¤±å‡½æ•°
-   æ¢¯åº¦ä¸‹é™æ³•


çº¿æ€§å¯åˆ†å’Œçº¿æ€§ä¸å¯åˆ†
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/D2mhQ0UiafgCuBRJ.png)
å…³äºŽ0-1åˆ†ç±»é—®é¢˜ï¼Œçº¿æ€§å›žå½’è§£å†³çš„ç»“æžœå¹¶ä¸ç†æƒ³
### Sigmod åˆ†å¸ƒå‡½æ•°
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/WtcHb7rYG2RjqnvB.png)
```
def sigmoid(z): 
	sigmoid = 1 / (1 + np.exp(-z)) 
	return sigmoid
```
å›¾åƒå¦‚ä¸‹ï¼š
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/UZJISGkd2jWNzPJ7.png)
ä¸Šå›¾å°±æ˜¯ Sigmoid å‡½æ•°çš„å›¾åƒï¼Œä½ ä¼šæƒŠè®¶åœ°å‘çŽ°ï¼Œè¿™ä¸ªå›¾åƒå‘ˆçŽ°å‡ºå®Œç¾Žçš„ S åž‹ï¼ˆSigmoid çš„å«ä¹‰ï¼‰ã€‚å®ƒçš„å–å€¼ä»…ä»‹äºŽ 0 å’Œ 1ä¹‹é—´ï¼Œä¸”å…³äºŽ ð‘§=0 è½´ä¸­å¿ƒå¯¹ç§°ã€‚åŒæ—¶å½“ ð‘§ è¶Šå¤§æ—¶ï¼Œð‘¦ è¶ŠæŽ¥è¿‘äºŽ 1ï¼Œè€Œ ð‘§ è¶Šå°æ—¶ï¼Œð‘¦ è¶ŠæŽ¥è¿‘äºŽ 0ã€‚å¦‚æžœæˆ‘ä»¬ä»¥ 0.5 ä¸ºåˆ†ç•Œç‚¹ï¼Œå°† >0.5 æˆ– <0.5 çš„å€¼åˆ†ä¸ºä¸¤ç±»ï¼Œè¿™ä¸å°±æ˜¯è§£å†³ 0âˆ’1 äºŒåˆ†ç±»é—®é¢˜çš„å®Œç¾Žé€‰æ‹©å˜›ã€‚
### é€»è¾‘å›žå½’æ¨¡åž‹
å¦‚æžœä¸€ç»„è¿žç»­éšæœºå˜é‡ç¬¦åˆ Sigmoid å‡½æ•°æ ·æœ¬åˆ†å¸ƒï¼Œå°±ç§°ä½œä¸ºé€»è¾‘åˆ†å¸ƒã€‚æŠŠçº¿æ€§å‡½æ•°æ‹Ÿåˆçš„ç»“æžœä½¿ç”¨ Sigmoid å‡½æ•°åŽ‹ç¼©åˆ° (0,1)ä¹‹é—´ã€‚å¦‚æžœçº¿æ€§å‡½æ•°çš„ ð‘¦å€¼è¶Šå¤§ï¼Œä¹Ÿå°±ä»£è¡¨æ¦‚çŽ‡è¶ŠæŽ¥è¿‘äºŽ 1ï¼Œåä¹‹æŽ¥è¿‘äºŽ 0ã€‚
#### å¯¹æ•°æŸå¤±å‡½æ•°
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/yms9QSUZc3XhbYcn.png)
```
def loss(h, y): 
	loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() 
	return loss
```
#### æ¢¯åº¦ä¸‹é™æ³•
æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ¢¯åº¦çš„æ–¹å‘æŒ‡å‘å‡½æ•°å€¼å¢žåŠ æœ€å¿«çš„æ–¹å‘ï¼Œæ¢¯åº¦çš„å€¼è¡¨ç¤ºå‡½æ•°å€¼å¢žåŠ çš„é€ŸçŽ‡ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¯¹äºŽä¸€å…ƒå‡½æ•°è€Œè¨€ï¼Œæ¢¯åº¦å°±æ˜¯æŒ‡åœ¨æŸä¸€ç‚¹çš„å¯¼æ•°ã€‚è€Œå¯¹äºŽå¤šå…ƒå‡½æ•°è€Œè¨€ï¼Œæ¢¯åº¦å°±æ˜¯æŒ‡åœ¨æŸä¸€ç‚¹çš„åå¯¼æ•°ç»„æˆçš„å‘é‡ã€‚

æ—¢ç„¶ï¼Œå‡½æ•°åœ¨æ²¿æ¢¯åº¦æ–¹å‘å˜åŒ–æœ€å¿«ï¼Œæ‰€ä»¥ã€Œæ¢¯åº¦ä¸‹é™æ³•ã€çš„æ ¸å¿ƒå°±æ˜¯ï¼Œæˆ‘ä»¬æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘åŽ»å¯»æ‰¾æŸå¤±å‡½æ•°çš„æžå°å€¼ã€‚è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/D2q6G3KHTq5WuhVF.png)

![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/AQ1mRoaFy30DGmlf.png)
```
def gradient(X, h, y): 
	gradient = np.dot(X.T, (h - y)) / y.shape[0] 
	return gradient
```

### é€»è¾‘å›žå½’çš„scikit-learnå®žçŽ°
åœ¨ scikit-learn ä¸­ï¼Œå®žçŽ°é€»è¾‘å›žå½’çš„ç±»åŠé»˜è®¤å‚æ•°æ˜¯ï¼š
`LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)`
ä»‹ç»å…¶ä¸­å‡ ä¸ªå¸¸ç”¨çš„å‚æ•°ï¼Œå…¶ä½™ä½¿ç”¨é»˜è®¤å³å¯ï¼š
-   `penalty`: æƒ©ç½šé¡¹ï¼Œé»˜è®¤ä¸º ð¿2 æ­£åˆ™ã€‚
-   `dual`: å¯¹å¶åŒ–ï¼Œé»˜è®¤ä¸º Falseã€‚
-   `tol`: æ”¶æ•›é˜ˆå€¼ï¼Œå½“æ¨¡åž‹å‚æ•°çš„æ›´æ–°é‡å°äºŽtolæ—¶ï¼Œè®¤ä¸ºæ¨¡åž‹å·²ç»æ”¶æ•›ï¼Œåœæ­¢è¿­ä»£ï¼ˆå³ä½¿æ²¡æœ‰è¾¾åˆ° `max_iter`ï¼‰ï¼Œé»˜è®¤å€¼ä¸º0.0001ã€‚
-   `fit_intercept`: é»˜è®¤ä¸º Trueï¼Œè®¡ç®—æˆªè·é¡¹ã€‚
-   `random_state`: éšæœºæ•°å‘ç”Ÿå™¨ã€‚
-   `max_iter`: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 100ã€‚

å¦å¤–ï¼Œ`solver` å‚æ•°ç”¨äºŽæŒ‡å®šæ±‚è§£æŸå¤±å‡½æ•°çš„æ–¹æ³•ã€‚é»˜è®¤ä¸º `liblinear`ï¼ˆ0.22 å¼€å§‹é»˜è®¤ä¸º `lbfgs`ï¼‰ï¼Œé€‚åˆäºŽå°æ•°æ®é›†ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰é€‚åˆå¤šåˆ†ç±»é—®é¢˜çš„ `newton-cg`, `sag`, `saga` å’Œ `lbfgs` æ±‚è§£å™¨ã€‚è¿™äº›æ–¹æ³•éƒ½æ¥è‡ªäºŽä¸€äº›å­¦æœ¯è®ºæ–‡ï¼Œæœ‰å…´è¶£å¯ä»¥è‡ªè¡Œæœç´¢äº†è§£ã€‚
```
from sklearn.linear_model import LogisticRegression

import matplotlib.pyplot as plt

import pandas as pd

import numpy as np

df = pd.read_csv(

"https://labfile.oss.aliyuncs.com/courses/1081/course-8-data.csv", header=0) # åŠ è½½æ•°æ®é›†

x = df[['X0', 'X1']].values

y = df['Y'].values

model = LogisticRegression(tol=0.001, max_iter=10000, solver='liblinear')

model.fit(x, y)

plt.figure(figsize=(10, 6))#åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£10*6è‹±å¯¸

plt.scatter(df['X0'], df['X1'], c=df['Y'])#ç”»æ•£ç‚¹å›¾ï¼Œcä¸ºç‰¹å¾å€¼ï¼Œè¡¨ç¤ºç‚¹çš„é¢œè‰²

  

x1_min, x1_max = df['X0'].min(), df['X0'].max()

x2_min, x2_max = df['X1'].min(), df['X1'].max()

  

xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max),

np.linspace(x2_min, x2_max))

grid = np.c_[xx1.ravel(), xx2.ravel()]
#è¿™è¡Œä»£ç ä½¿ç”¨ `np.meshgrid()` å‡½æ•°åˆ›å»ºäº†ä¸€ä¸ªäºŒç»´ç½‘æ ¼ï¼Œå…¶ä¸­ `xx1` å¯¹åº”äºŽ x è½´ä¸Šçš„ç‚¹ï¼Œ`xx2` å¯¹åº”äºŽ y è½´ä¸Šçš„ç‚¹ã€‚`np.linspace()` å‡½æ•°ç”¨äºŽåœ¨æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´ç”Ÿæˆç­‰é—´éš”çš„ç‚¹ã€‚
  

probs = (np.dot(grid, model.coef_.T) + model.intercept_).reshape(xx1.shape)
#  `grid = np.c_[xx1.ravel(), xx2.ravel()]`: å°†äºŒç»´ç½‘æ ¼å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ï¼Œå¹¶æŒ‰åˆ—è¿žæŽ¥ï¼Œå½¢æˆä¸€ä¸ªçŸ©é˜µã€‚è¿™æ ·æ¯ä¸ªç‚¹çš„åæ ‡å°±å¯ä»¥ç”¨ä¸€å¯¹ (x, y) çš„å½¢å¼è¡¨ç¤ºã€‚

plt.contour(xx1, xx2, probs, levels=[0], linewidths=1, colors='red')
#è¿™è¡Œä»£ç ä½¿ç”¨ `plt.contour()` å‡½æ•°ç»˜åˆ¶ç­‰é«˜çº¿å›¾ï¼Œå³å†³ç­–è¾¹ç•Œã€‚`xx1` å’Œ `xx2` æ˜¯ç½‘æ ¼ç‚¹çš„åæ ‡ï¼Œ`probs` æ˜¯å¯¹åº”çš„æ¦‚çŽ‡å€¼ã€‚`levels=[0]` è¡¨ç¤ºåªç»˜åˆ¶æ¦‚çŽ‡ä¸º 0 çš„ç­‰é«˜çº¿ï¼Œå³å†³ç­–è¾¹ç•Œã€‚`linewidths=1` è®¾ç½®ç­‰é«˜çº¿çš„å®½åº¦ä¸º 1ï¼Œ`colors='red'` è®¾ç½®ç­‰é«˜çº¿çš„é¢œè‰²ä¸ºçº¢è‰²ã€‚

model.score(x, y)#æ¨¡åž‹åœ¨è®­ç»ƒé›†ä¸Šçš„åˆ†ç±»å‡†ç¡®çŽ‡
```
![è¾“å…¥å›¾ç‰‡è¯´æ˜Ž](/imgs/2024-04-29/2RCGvfXrSMQ9C8Ja.png)


##  ç¤ºä¾‹

Logisticå›žå½’æ¨¡åž‹ã€‚è®©æˆ‘ä»¬é€ä¸ªå‚æ•°æ¥è§£é‡Šï¼š

1. `model = LogisticRegression(tol=0.001, max_iter=10000, solver='liblinear')`:
   - `LogisticRegression`ï¼šè¿™æ˜¯ä»Ž`sklearn.linear_model`æ¨¡å—ä¸­å¯¼å…¥çš„ä¸€ä¸ªç±»ï¼Œç”¨äºŽå®žçŽ°é€»è¾‘å›žå½’æ¨¡åž‹ã€‚
   - `tol=0.001`ï¼šè¿™æ˜¯æ¨¡åž‹çš„å®¹å·®å‚æ•°ã€‚å®ƒæŒ‡å®šäº†æ¨¡åž‹æ”¶æ•›çš„æ ‡å‡†ã€‚å½“æ¨¡åž‹çš„ä¼˜åŒ–ç®—æ³•çš„å˜åŒ–é‡å°äºŽè¿™ä¸ªå€¼æ—¶ï¼Œä¼˜åŒ–è¿‡ç¨‹å°†åœæ­¢ã€‚è¿™æ„å‘³ç€æ¨¡åž‹åœ¨æ‰¾åˆ°æŸå¤±å‡½æ•°çš„æžå°å€¼æ—¶ä¼šåœæ­¢è¿­ä»£ã€‚
   - `max_iter=10000`ï¼šè¿™æ˜¯æ¨¡åž‹æœ€å¤§è¿­ä»£æ¬¡æ•°å‚æ•°ã€‚å®ƒå®šä¹‰äº†æ¨¡åž‹ä¼˜åŒ–ç®—æ³•å¯ä»¥æ‰§è¡Œçš„æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚è®¾ç½®è¾ƒé«˜çš„è¿­ä»£æ¬¡æ•°å¯ä»¥ç¡®ä¿åœ¨è¾ƒå¤æ‚çš„æ•°æ®é›†ä¸Šæ¨¡åž‹æœ‰è¶³å¤Ÿçš„æœºä¼šæ”¶æ•›ã€‚
   - `solver='liblinear'`ï¼šè¿™æ˜¯æŒ‡å®šç”¨äºŽä¼˜åŒ–çš„ç®—æ³•ã€‚`liblinear`æ˜¯ä¸€ä¸ªä¼˜åŒ–ç®—æ³•ï¼Œé€‚ç”¨äºŽå°æ•°æ®é›†æˆ–ç¨€ç–æ•°æ®ã€‚å®ƒæ˜¯ä¸€ä¸ªåŸºäºŽå•å˜é‡çº¿æ€§åˆ†ç±»å™¨çš„åº“ï¼Œé€‚åˆé€»è¾‘å›žå½’ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªé€»è¾‘å›žå½’æ¨¡åž‹ï¼Œå…¶æ”¶æ•›å®¹å·®ä¸º0.001ï¼Œæœ€å¤§è¿­ä»£æ¬¡æ•°ä¸º10000æ¬¡ï¼Œå¹¶ä½¿ç”¨`liblinear`ä¼˜åŒ–ç®—æ³•è¿›è¡Œè®­ç»ƒã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼ŒåŒ…æ‹¬å¦‚ä½•å¯¼å…¥åº“ã€åˆ›å»ºæ¨¡åž‹ã€è®­ç»ƒæ¨¡åž‹å’Œè¿›è¡Œé¢„æµ‹ï¼š

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# åˆ›å»ºä¸€ä¸ªåˆ†ç±»æ•°æ®é›†
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åˆ›å»ºé€»è¾‘å›žå½’æ¨¡åž‹
model = LogisticRegression(tol=0.001, max_iter=10000, solver='liblinear')

# è®­ç»ƒæ¨¡åž‹
model.fit(X_train, y_train)

# è¿›è¡Œé¢„æµ‹
predictions = model.predict(X_test)

# æ‰“å°é¢„æµ‹ç»“æžœ
print(predictions)
```

é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œä½ å¯ä»¥çœ‹åˆ°å¦‚ä½•åˆ›å»ºå¹¶è®­ç»ƒé€»è¾‘å›žå½’æ¨¡åž‹ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡åž‹è¿›è¡Œé¢„æµ‹ã€‚
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg3NDM5NTgwOV19
-->