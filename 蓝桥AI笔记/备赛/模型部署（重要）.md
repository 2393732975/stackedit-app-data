## 模型部署
[TOC]
### ONNX 模型转换
[官方文档：9.1 使用ONNX进行部署并推理 — 深入浅出PyTorch (datawhalechina.github.io)](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86.html)
#### 模型转化
把 PyTorch 模型转换成 ONNX 模型时，我们往往只需要轻松地调用一句torch.onnx.export就可以了。这个函数的接口看上去简单，但它在使用上还有着诸多的注意事项。

前三个必选参数为模型、模型输入、导出的 onnx 文件名，我们对这几个参数已经很熟悉了。我们来着重看一下后面的一些常用可选参数。

**export_params**
模型中是否存储模型权重。一般中间表示包含两大类信息：模型结构和模型权重，这两类信息可以在同一个文件里存储，也可以分文件存储。ONNX 是用同一个文件表示记录模型的结构和权重的。
我们部署时一般都默认这个参数为 True。如果 onnx 文件是用来在不同框架间传递模型（比如 PyTorch 到 Tensorflow）而不是用于部署，则可以令这个参数为 False。
**input_names, output_names**
设置输入和输出张量的名称。如果不设置的话，会自动分配一些简单的名字（如数字）。
ONNX 模型的每个输入和输出张量都有一个名字。很多推理引擎在运行 ONNX 文件时，都需要以“名称-张量值”的数据对来输入数据，并根据输出张量的名称来获取输出数据。在进行跟张量有关的设置（比如添加动态维度）时，也需要知道张量的名字。
在实际的部署流水线中，我们都需要设置输入和输出张量的名称，并保证 ONNX 和推理引擎中使用同一套名称。
**opset_version**
转换时参考哪个 ONNX 算子集版本，默认为 9。后文会详细介绍 PyTorch 与 ONNX 的算子对应关系。
dynamic_axes
指定输入输出张量的哪些维度是动态的。

```python
import torch

# 这一块区域为模型加载的步骤具体可以依据自己使用情况替换
from model import efficientnetv2_s as create_model
device = "cpu"
model = create_model(num_classes=2).to(device)
model_weight_path = "./weights1/model-54.pth"
model.load_state_dict(torch.load(model_weight_path, map_location=device))
model.eval()



batch_size = 1  # 批处理大小
input_shape = (3, 224, 224)  # 输入数据

x = torch.randn(batch_size, *input_shape)  # 生成张量
export_onnx_file = "test.onnx"  # 目的ONNX文件名
torch.onnx.export(model,
                  x,
                  export_onnx_file,
                  opset_version=10,
                  do_constant_folding=True,  # 是否执行常量折叠优化
                  input_names=["input"],  # 输入名
                  output_names=["output"],  # 输出名
                  dynamic_axes={"input": {0: "batch_size"},  # 批处理变量
                                "output": {0: "batch_size"}})

```

#### 使用onnx推理

```python
import os, sys

sys.path.append(os.getcwd())
import onnxruntime
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image


def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()

# 自定义的数据增强
def get_test_transform(): 
    return transforms.Compose([
        transforms.Resize([224, 224]),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

# 推理的图片路径
image = Image.open('./0a0b8641cac0ce40315e38af020bb18f-device4-0-f_items6.jpg').convert('RGB')

img = get_test_transform()(image)
img = img.unsqueeze_(0)  # -> NCHW, 1,3,224,224
# 模型加载
onnx_model_path = "test.onnx"
resnet_session = onnxruntime.InferenceSession(onnx_model_path)
inputs = {resnet_session.get_inputs()[0].name: to_numpy(img)}
outs = resnet_session.run(None, inputs)[0]

print("onnx weights", outs)
print("onnx prediction", outs.argmax(axis=1)[0])

```

### Tensor Flow Lite 模型转换
[转换 TensorFlow 模型 | TensorFlow Lite (google.cn)](https://tensorflow.google.cn/lite/models/convert/convert_models?hl=zh-cn)

```python
# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
# Save the model.
with open('model.tflite', 'wb') as f:  
	f.write(tflite_model)
```

### Flask 框架应用
一个最小的Flask框架应用：
```python
from flask import Flask

# 实例化创建一个 Flask 应用，第一个参数是 Flask 应用的名称。__name__ 是一个标识 Python 模块的名字的变量
app = Flask(__name__)

'''定义函数 hello_world，它返回一段 html 文本。
app.route(’/’) 返回一个装饰器，装饰器来为函数 hello_world 绑定对应的 URL，
当用户在浏览器访问这个 URL 的时候，就会触发这个函数，获取返回值。'''
@app.route('/')
def hello_word():
    return '<b>Hello Word</b>'

# 如果当前模块是主模块，则变量 __name__ 为 '__main__，此时调用 run() 方法启动 Flask 应用
if __name__ == '__main__':
    app.run()
```
 **修改监听地址和端口**
 ```python
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return '<b>Hello World</b>'

if __name__ == '__main__':
    app.run(host = '0.0.0.0', port = 8888)
```
设定 app.run 的函数参数 host 为 ‘0.0.0.0’，表示监听每一个可用的网络接口；设定 app.run 的函数参数 port 为 8888，表示监听端口 8888。在浏览器输入127.0.0.1:8889 或者 localhost:8889.

**调试模式**
在代码中，在运行或将调试参数传递给run()方法之前，通过将application对象的debug属性设置为True来启用Debug模式。
```python
app.debug = True
app.run()
# 或者
app.run(debug = True)
```

**Flask 路由**
Flask中的route()装饰器用于将URL绑定到函数。
```python
@app.route('/hello')
def hello_world():
   return 'hello world'
```
在这里，URL`'/ hello'`规则绑定到hello_world()函数。 因此，如果用户访问`http://localhost:5000/hello` URL，hello_world()函数的输出将在浏览器中呈现。

application对象的add_url_rule()函数也可用于将URL与函数绑定，如上例所示，使用route()装饰器的目的也由以下表示：
```python
def hello_world():
   return 'hello world'
app.add_url_rule('/', 'hello', hello_world)
```

**Flask变量规则**
通过向规则参数添加变量部分，可以动态构建URL。此变量部分标记为`<converter:variable_name>`。它作为关键字参数传递给与规则相关联的函数。  
在以下示例中，route()装饰器的规则参数包含附加到URL`'/hello'`的`<name>`。 因此，如果在浏览器中输入http://localhost:5000/hello/George作为URL，则`'George'`将作为参数提供给 hello()函数。
```python
from flask import Flask
 
app = Flask(__name__)
 
@app.route('/hello/<name>')
def hello_name(name):
    return 'Hello %s!' % name
 
if __name__ == '__main__':
    app.run(debug=True)
```
![输入图片说明](/imgs/2024-05-21/Oi658b43VETUDVtG.png)

**唯一的 URL / 重定向行为**

以下两条规则的不同之处在于是否使用尾部的斜杠。:

```python
@app.route('/projects/')
def projects():
    return 'The project page'
 
@app.route('/about')
def about():
    return 'The about page'
```

projects 的 URL 是中规中矩的，尾部有一个斜杠，看起来就如同一个文件夹。 访问一个没有斜杠结尾的 URL 时 Flask 会自动进行重定向，帮你在尾部加上一个斜杠。

about 的 URL 没有尾部斜杠，因此其行为表现与一个文件类似。如果访问这个 URL 时添加了尾部斜杠就会得到一个 404 错误。这样可以保持 URL 唯一，并帮助 搜索引擎避免重复索引同一页面。

### Gradio 模型部署


### 模型压缩


### 模型量化


### 模型输出后处理





<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkxNDM4ODI4NywyMTU4MDMzNTQsNjc4Mj
MzODc1LC0xNjQyODkxOTY4LC0xMjQxNjg4NjY2XX0=
-->