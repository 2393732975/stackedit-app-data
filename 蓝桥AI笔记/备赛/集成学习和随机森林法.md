


é›†æˆï¼ˆEnsembleï¼‰ã€‚é›†æˆæ˜¯ä½¿ç”¨ä¸€ç³»åˆ—å­¦ä¹ å™¨è¿›è¡Œå­¦ä¹ ï¼Œå¹¶ä½¿ç”¨æŸç§è§„åˆ™æŠŠå„ä¸ªå­¦ä¹ ç»“æœè¿›è¡Œæ•´åˆä»è€Œè·å¾—æ¯”å•ä¸ªå­¦ä¹ å™¨æ›´å¥½çš„å­¦ä¹ æ•ˆæœçš„ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚åœ¨é›†æˆä¸­ï¼Œæœ€ç»ˆçš„æ•´ä½“è¾“å‡ºæ¯”ä»»ä½•å•ä¸ªéƒ¨åˆ†çš„è¡¨ç°æ›´é‡è¦ã€‚
### Bootstrapping
Leo Breiman äº 1994 å¹´æå‡ºçš„ Baggingï¼ˆåˆç§° Bootstrap Aggregationï¼Œå¼•å¯¼èšé›†ï¼‰æ˜¯æœ€åŸºæœ¬çš„é›†æˆæŠ€æœ¯ä¹‹ä¸€ã€‚Bagging åŸºäºç»Ÿè®¡å­¦ä¸­çš„ Bootstrapingï¼ˆè‡ªåŠ©æ³•ï¼‰ï¼Œè¯¥æ–¹æ³•ä»¤å¤æ‚æ¨¡å‹çš„ç»Ÿè®¡è¯„ä¼°å˜å¾—æ›´åŠ å¯è¡Œã€‚

Bootstrap æ–¹æ³•çš„æµç¨‹å¦‚ä¸‹ï¼šå‡è®¾æœ‰å°ºå¯¸ä¸º N çš„æ ·æœ¬ Xï¼Œä»è¯¥æ ·æœ¬ä¸­æœ‰æ”¾å›åœ°éšæœºæŠ½å– N ä¸ªæ ·æœ¬ï¼Œä»¥åˆ›å»ºä¸€ä¸ªæ–°æ ·æœ¬ã€‚æ¢å¥è¯è¯´ï¼Œä»å°ºå¯¸ä¸º N çš„åŸæ ·æœ¬ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶é‡å¤æ­¤è¿‡ç¨‹ N æ¬¡ã€‚é€‰ä¸­æ‰€æœ‰å…ƒç´ çš„å¯èƒ½æ€§æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤æ¯ä¸ªå…ƒç´ è¢«æŠ½ä¸­çš„æ¦‚ç‡å‡ä¸º 1/ğ‘â€‹ã€‚

### Bagging
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒé›† Xã€‚æˆ‘ä»¬ä½¿ç”¨ Bootstrap ç”Ÿæˆæ ·æœ¬ ğ‘‹1,â€¦,ğ‘‹ğ‘€X1â€‹,â€¦,XMâ€‹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ª Bootstrap æ ·æœ¬ä¸Šåˆ†åˆ«è®­ç»ƒåˆ†ç±»å™¨ ğ‘ğ‘–(ğ‘¥)aiâ€‹(x)ï¼Œæœ€ç»ˆçš„åˆ†ç±»å™¨å°†å¯¹æ‰€æœ‰è¿™äº›å•ç‹¬çš„åˆ†ç±»å™¨çš„è¾“å‡ºå–å‡å€¼ã€‚åœ¨åˆ†ç±»æƒ…å½¢ä¸‹ï¼Œè¿™ç§æŠ€æœ¯å³æŠ•ç¥¨ï¼ˆvotingï¼‰ï¼š
![è¾“å…¥å›¾ç‰‡è¯´æ˜](/imgs/2024-05-01/WsVieTVxcKm57fUu.png)

ä¸‹å›¾å½¢è±¡åŒ–è§£é‡Šäº†ä¸Šé¢çš„å…¬å¼ï¼š
![è¾“å…¥å›¾ç‰‡è¯´æ˜](/imgs/2024-05-01/Ez15xZwfYqntjWIz.png)

## éšæœºæ£®æ—

Scikit-Learnä¸­çš„éšæœºæ£®æ—
Scikit-Learnæä¾›äº†ä¸¤ä¸ªå®ç°éšæœºæ£®æ—æ¨¡å‹çš„ç±»ï¼š

RandomForestClassifier ç”¨äºåˆ†ç±»é—®é¢˜ã€‚

RandomForestRegressor ç”¨äºå›å½’é—®é¢˜ã€‚

é™¤äº†å†³ç­–æ ‘çš„æ ‡å‡†è¶…å‚æ•°ï¼ˆå¦‚criterionå’Œmax_depthï¼‰ï¼Œè¿™äº›ç±»è¿˜æä¾›ä»¥ä¸‹è¶…å‚æ•°ï¼š

n_estimators - æ£®æ—ä¸­æ ‘çš„æ•°é‡ï¼ˆé»˜è®¤ä¸º100ï¼‰ã€‚

max_features - åœ¨æ¯ä¸ªèŠ‚ç‚¹æœç´¢æœ€ä½³åˆ†å‰²æ—¶è¦è€ƒè™‘çš„ç‰¹å¾æ•°é‡ã€‚é€‰é¡¹æ˜¯æŒ‡å®šä¸€ä¸ªæ•´æ•°è¡¨ç¤ºç‰¹å¾æ•°é‡ï¼Œä¸€ä¸ªæµ®ç‚¹æ•°è¡¨ç¤ºè¦ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹ï¼Œâ€˜sqrtâ€™ï¼ˆé»˜è®¤å€¼ï¼‰è¡¨ç¤ºä½¿ç”¨ç‰¹å¾çš„å¹³æ–¹æ ¹ï¼Œ'log2â€™è¡¨ç¤ºä½¿ç”¨ç‰¹å¾çš„log2ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ç‰¹å¾ã€‚

max_samples - ä»è®­ç»ƒé›†ä¸­æŠ½å–ç”¨äºè®­ç»ƒæ¯æ£µæ ‘çš„æ ·æœ¬æ•°é‡ã€‚é€‰é¡¹æ˜¯æŒ‡å®šä¸€ä¸ªæ•´æ•°è¡¨ç¤ºæ ·æœ¬æ•°é‡ï¼Œä¸€ä¸ªæµ®ç‚¹æ•°è¡¨ç¤ºè¦ä½¿ç”¨çš„è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼ŒNoneï¼ˆé»˜è®¤å€¼ï¼‰è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ã€‚

æ­¤å¤–ï¼Œä¸ºäº†åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°n_jobså¹¶è¡Œè®­ç»ƒæ ‘ï¼Œè¯¥å‚æ•°æŒ‡å®šç”¨äºè®­ç»ƒçš„CPUæ ¸å¿ƒæ•°ï¼ˆé»˜è®¤å€¼ä¸º1ï¼‰ã€‚å¦‚æœn_jobs = -1ï¼Œåˆ™å°†ä½¿ç”¨æœºå™¨ä¸Šæ‰€æœ‰å¯ç”¨çš„æ ¸å¿ƒè¿›è¡Œè®­ç»ƒã€‚
[æœºå™¨å­¦ä¹ æ¨¡å‹ç³»åˆ—ï¼šéšæœºæ£®æ—çš„åŸç†å’Œç¤ºä¾‹ä»‹ç»_éšæœºæ£®æ—æ¨¡å‹-CSDNåšå®¢](https://blog.csdn.net/wjjc1017/article/details/135904420)

### RandomForestClassifier è§£å†³åˆ†ç±»é—®é¢˜
```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()

# åªå–å‰ä¸¤ä¸ªç‰¹å¾
X = iris.data[:, :2]

# ç›®æ ‡å˜é‡
y = iris.target 
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

params = {

'n_estimators': [10, 50, 100, 200, 500], # å†³ç­–æ ‘çš„æ•°é‡

'max_depth': np.arange(3, 11), # å†³ç­–æ ‘çš„æœ€å¤§æ·±åº¦èŒƒå›´

'max_samples': np.arange(0.5, 1.0, 0.1), # æ¯æ£µå†³ç­–æ ‘çš„æ ·æœ¬æ¯”ä¾‹èŒƒå›´

'max_features': ['sqrt', 'log2', None] # æ¯æ£µå†³ç­–æ ‘çš„ç‰¹å¾é€‰æ‹©æ–¹å¼

}

search = RandomizedSearchCV(RandomForestClassifier(random_state=42), params, n_iter=50, cv=3, n_jobs=-1) # éšæœºæœç´¢äº¤å‰éªŒè¯

search.fit(X_train, y_train) # æ‹Ÿåˆè®­ç»ƒæ•°æ®


print(search.best_params_) # è¾“å‡ºæœ€ä½³å‚æ•°
 

# å°†æœ€ä½³åˆ†ç±»å™¨èµ‹å€¼ç»™best_clfå˜é‡
best_clf = search.best_estimator_

# è¾“å‡ºè®­ç»ƒé›†ä¸Šçš„å‡†ç¡®ç‡
print(f'Train accuracy: {best_clf.score(X_train, y_train):.4f}')

# è¾“å‡ºæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡
print(f'Test accuracy: {best_clf.score(X_test, y_test):.4f}')
```
### RandomForestRegressor è§£å†³åˆ†ç±»é—®é¢˜
çœ‹csdné“¾æ¥å³å¯ï¼Œä¸ä¸Šé¢çš„å¤§å·®ä¸å·®


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkyNTc1OTcwXX0=
-->