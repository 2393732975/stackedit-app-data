


集成（Ensemble）。集成是使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。在集成中，最终的整体输出比任何单个部分的表现更重要。
### Bootstrapping
Leo Breiman 于 1994 年提出的 Bagging（又称 Bootstrap Aggregation，引导聚集）是最基本的集成技术之一。Bagging 基于统计学中的 Bootstraping（自助法），该方法令复杂模型的统计评估变得更加可行。

Bootstrap 方法的流程如下：假设有尺寸为 N 的样本 X，从该样本中有放回地随机抽取 N 个样本，以创建一个新样本。换句话说，从尺寸为 N 的原样本中随机选择一个元素，并重复此过程 N 次。选中所有元素的可能性是一样的，因此每个元素被抽中的概率均为 1/𝑁​。

### Bagging
假设我们有一个训练集 X。我们使用 Bootstrap 生成样本 𝑋1,…,𝑋𝑀X1​,…,XM​。现在，我们在每个 Bootstrap 样本上分别训练分类器 𝑎𝑖(𝑥)ai​(x)，最终的分类器将对所有这些单独的分类器的输出取均值。在分类情形下，这种技术即投票（voting）：
![输入图片说明](/imgs/2024-05-01/WsVieTVxcKm57fUu.png)

下图形象化解释了上面的公式：
![输入图片说明](/imgs/2024-05-01/Ez15xZwfYqntjWIz.png)

## 随机森林

Scikit-Learn中的随机森林
Scikit-Learn提供了两个实现随机森林模型的类：

RandomForestClassifier 用于分类问题。

RandomForestRegressor 用于回归问题。

除了决策树的标准超参数（如criterion和max_depth），这些类还提供以下超参数：

n_estimators - 森林中树的数量（默认为100）。

max_features - 在每个节点搜索最佳分割时要考虑的特征数量。选项是指定一个整数表示特征数量，一个浮点数表示要使用的特征比例，‘sqrt’（默认值）表示使用特征的平方根，'log2’表示使用特征的log2，None表示使用所有特征。

max_samples - 从训练集中抽取用于训练每棵树的样本数量。选项是指定一个整数表示样本数量，一个浮点数表示要使用的训练样本的比例，None（默认值）表示使用所有训练样本。

此外，为了加快训练速度，您可以使用参数n_jobs并行训练树，该参数指定用于训练的CPU核心数（默认值为1）。如果n_jobs = -1，则将使用机器上所有可用的核心进行训练。
[机器学习模型系列：随机森林的原理和示例介绍_随机森林模型-CSDN博客](https://blog.csdn.net/wjjc1017/article/details/135904420)

### RandomForestClassifier 解决分类问题
```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# 加载鸢尾花数据集
iris = load_iris()

# 只取前两个特征
X = iris.data[:, :2]

# 目标变量
y = iris.target 
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

params = {

'n_estimators': [10, 50, 100, 200, 500], # 决策树的数量

'max_depth': np.arange(3, 11), # 决策树的最大深度范围

'max_samples': np.arange(0.5, 1.0, 0.1), # 每棵决策树的样本比例范围

'max_features': ['sqrt', 'log2', None] # 每棵决策树的特征选择方式

}

search = RandomizedSearchCV(RandomForestClassifier(random_state=42), params, n_iter=50, cv=3, n_jobs=-1) # 随机搜索交叉验证

search.fit(X_train, y_train) # 拟合训练数据


print(search.best_params_) # 输出最佳参数
 

# 将最佳分类器赋值给best_clf变量
best_clf = search.best_estimator_

# 输出训练集上的准确率
print(f'Train accuracy: {best_clf.score(X_train, y_train):.4f}')

# 输出测试集上的准确率
print(f'Test accuracy: {best_clf.score(X_test, y_test):.4f}')
```
### RandomForestRegressor 解决分类问题
看csdn链接即可，与上面的大差不差


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkyNTc1OTcwXX0=
-->