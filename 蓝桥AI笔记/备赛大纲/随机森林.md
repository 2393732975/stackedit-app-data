## 随机森林

Scikit-Learn中的随机森林
Scikit-Learn提供了两个实现随机森林模型的类：

RandomForestClassifier 用于分类问题。

RandomForestRegressor 用于回归问题。

除了决策树的标准超参数（如criterion和max_depth），这些类还提供以下超参数：

n_estimators - 森林中树的数量（默认为100）。

max_features - 在每个节点搜索最佳分割时要考虑的特征数量。选项是指定一个整数表示特征数量，一个浮点数表示要使用的特征比例，‘sqrt’（默认值）表示使用特征的平方根，'log2’表示使用特征的log2，None表示使用所有特征。

max_samples - 从训练集中抽取用于训练每棵树的样本数量。选项是指定一个整数表示样本数量，一个浮点数表示要使用的训练样本的比例，None（默认值）表示使用所有训练样本。

此外，为了加快训练速度，您可以使用参数n_jobs并行训练树，该参数指定用于训练的CPU核心数（默认值为1）。如果n_jobs = -1，则将使用机器上所有可用的核心进行训练。
[机器学习模型系列：随机森林的原理和示例介绍_随机森林模型-CSDN博客](https://blog.csdn.net/wjjc1017/article/details/135904420)
### RandomForestClassifier 解决分类问题
```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# 加载鸢尾花数据集
iris = load_iris()

# 只取前两个特征
X = iris.data[:, :2]

# 目标变量
y = iris.target 
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

params = {

'n_estimators': [10, 50, 100, 200, 500], # 决策树的数量

'max_depth': np.arange(3, 11), # 决策树的最大深度范围

'max_samples': np.arange(0.5, 1.0, 0.1), # 每棵决策树的样本比例范围

'max_features': ['sqrt', 'log2', None] # 每棵决策树的特征选择方式

}

search = RandomizedSearchCV(RandomForestClassifier(random_state=42), params, n_iter=50, cv=3, n_jobs=-1) # 随机搜索交叉验证

search.fit(X_train, y_train) # 拟合训练数据


print(search.best_params_) # 输出最佳参数
 

# 将最佳分类器赋值给best_clf变量
best_clf = search.best_estimator_

# 输出训练集上的准确率
print(f'Train accuracy: {best_clf.score(X_train, y_train):.4f}')

# 输出测试集上的准确率
print(f'Test accuracy: {best_clf.score(X_test, y_test):.4f}')
```


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg0MzQ2MjMwMl19
-->