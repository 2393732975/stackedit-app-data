
### 决策树

```
# 导入所需的库
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
 
# 生成示例数据集
np.random.seed(0)
X = np.random.rand(100, 1) * 10  # 生成100个0到10之间的随机数作为特征
y = 2 * X.squeeze() + np.random.randn(100)  # 生成对应的目标值，y = 2 * x + 噪声
 
# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
# 创建并训练决策树回归模型
regressor = DecisionTreeRegressor(max_depth=3)  # 设置决策树的最大深度为3
regressor.fit(X_train, y_train)
 
# 使用训练好的模型进行预测
y_pred_train = regressor.predict(X_train)
y_pred_test = regressor.predict(X_test)
 
# 计算训练集和测试集的均方误差
mse_train = mean_squared_error(y_train, y_pred_train)
mse_test = mean_squared_error(y_test, y_pred_test)
 
print("训练集上的均方误差:", mse_train)
print("测试集上的均方误差:", mse_test)
 
# 绘制决策树回归模型在训练集上的拟合情况
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, color='blue', label='Training data')
plt.scatter(X_test, y_test, color='green', label='Testing data')
plt.plot(np.sort(X_train, axis=0), regressor.predict(np.sort(X_train, axis=0)), color='red', linewidth=2, label='Decision Tree Regression')
plt.title('Decision Tree Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.show()
```

sklearn.tree.DecisionTreeClassifier 类的主要参数为：

-   max_depth 树的最大深度；
-   max_features 搜索最佳分区时的最大特征数（特征很多时，设置这个参数很有必要，因为基于所有特征搜索分区会很「昂贵」）；
-   min_samples_leaf 叶节点的最少样本数。

树的参数需要根据输入数据设定，通常通过交叉验证可以确定参数范围，下文会具体讨论交叉验证。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2MzUyMTU1NjUsMTU3MzQ2NDc3MF19
-->