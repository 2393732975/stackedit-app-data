

考点：
数据(数值、文本、图像等)清洗、异常值检测与处理、数据转换、数据标准化/归一化、数据不均衡处理特征提取等数据处理技术。

### 数据清洗
主要分为这几个部分：
1. 处理缺失值 : `fillna（）`填补法

`dropna（）`删除法，特别注意：
`df.dropna(axis=0, how='any', inplace=True)`
-   `axis=0` 表示删除行，`axis=1` 表示删除列。
-   `how='any'` 表示只要该行（或列）中有任何一个缺失值就删除，`how='all'` 表示只有当该行（或列）所有值都是缺失值时才删除。
-   `inplace=True` 表示在原 DataFrame 上直接进行修改，而不返回一个新的 DataFrame。


2.  处理异常值 : `df['salary'] = df['salary'].apply(lambda x: x if 30000 <= x <= 80000 else df['salary'].median()) `
需要记住apply的用法以及lambda表达式
3. 处理重复值 
`# 删除'name'和'city'列上重复的行，只保留第一次出现的行  
df.drop_duplicates(subset=['name', 'city'], keep='first', inplace=True)  `


```text
import pandas as pd  
import numpy as np  
  
# 创建模拟数据，包含重复的行（基于'name'和'city'列）  
data = {  
    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Bob', 'David', 'Eve'],  
    'age': [25, np.nan, 35, 40, 45, 30, 40, 45],  
    'salary': [50000, 60000, np.nan, 75000, 80000, 60000, 120000, 80000],  
    'department': ['HR', 'IT', 'Sales', 'Finance', 'Marketing', 'IT', 'Finance', 'Marketing'],  
    'city': ['New York', 'San Francisco', 'Los Angeles', 'Chicago', 'Boston', 'San Francisco', 'Chicago', 'Boston']  
}  
  
# 创建DataFrame  
df = pd.DataFrame(data)  
  
# 查看原始数据  
print("原始数据:")  
print(df)  
  
# 处理缺失值  
# 使用列的均值填充'age'列的缺失值，使用列的中位数填充'salary'列的缺失值  
df['age'].fillna(df['age'].mean(), inplace=True)  
df['salary'].fillna(df['salary'].median(), inplace=True)  

# 处理异常值  
# 假设'salary'列中的值应该在30000到80000之间，我们将其超出这个范围的值视为异常值  
# 并用该列的中位数来替换这些异常值  
df['salary'] = df['salary'].apply(lambda x: x if 30000 <= x <= 80000 else df['salary'].median()) 
  
# 处理重复值  
# 删除'name'和'city'列上重复的行，只保留第一次出现的行  
df.drop_duplicates(subset=['name', 'city'], keep='first', inplace=True)  
  
# 查看清洗后的数据  
print("\n清洗后的数据:")  
print(df)
```
### 异常值检测与处理
主要是介绍z_score的异常值处理方法：

头文件：`from scipy import stats`

函数：`stats.zscore`

具体如下：
```
import pandas as pd
import numpy as np
from scipy import stats
# 创建一个包含异常值的数据集
data = {
'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Bob', 'David', 'Eve'],
'age': [25, np.nan, 35, 40, 45, 30, 40, 45],
'salary': [1, 2, 3, 4, 5, 6, 7, 100],
'department': ['HR', 'IT', 'Sales', 'Finance', 'Marketing', 'IT', 'Finance', 'Marketing'],
'city': ['New York', 'San Francisco', 'Los Angeles', 'Chicago', 'Boston', 'San Francisco', 'Chicago', 'Boston']
}
# 创建DataFrame
df = pd.DataFrame(data)
z_scores = np.abs(stats.zscore(df['salary']))
# 定义阈值，通常设为 2或3
threshold = 2
# 找出超过阈值的异常值索引
outlier_indices = np.where(z_scores > threshold)
# 删除异常值所在的行
cleaned_df = df.drop(outlier_indices[0])
cleaned_df
```

### 数据转换
在构造特征的过程中，为了提高机器学习模型的性能和稳定性，通常需要对数据进行特征缩放（如标准化或归一化）、特征编码（如标签编码或 One-Hot 编码）等操作。这样做的目的是将原始数据转化为更有意义的特征，从而使机器学习算法能够更好地理解和利用这些特征来做出准确的预测。

下面是几种常用的特征处理方法：

**数值标准化/归一化**

数值标准化和归一化是特征工程中常用的方法，用于将数值型特征的取值范围归一到一个共同的尺度或范围内，以便更好地处理和分析数据。

#### Z-score 标准化：

Z-score 标准化（Standardization）通过将每个特征值减去该特征的均值，并除以标准差，得到标准化后的特征值。这种方法使得每个特征的均值为 0，标准差为 1。具体计算公式如下：

`𝑧=𝑥−𝜇𝜎`

其中， 𝑥 是原始特征值， 𝜇 是特征的均值， 𝜎 是特征的标准差， 𝑧 是标准化后的特征值。

Z-score 标准化的优点是对数据的分布具有平移和缩放不变性，即标准化后的数据不受数据整体偏移和缩放的影响。

```text
from sklearn.preprocessing import StandardScaler
import numpy as np

# 示例数据
data = np.array([[1, 2], [3, 4], [5, 6]])
print("标准化前的数据：\n", data)

# 创建标准化对象
scaler = StandardScaler()

# 对数据进行标准化
standardized_data = scaler.fit_transform(data)
print("标准化后的数据：\n", standardized_data)
```
输出：
```text
标准化前的数据：
 [[1 2]
 [3 4]
 [5 6]]
标准化后的数据：
 [[-1.22474487 -1.22474487]
 [ 0.          0.        ]
 [ 1.22474487  1.22474487]]
```

#### Min-Max归一化
Min-Max归一化（Normalization）将每个特征值映射到一个指定的范围内，通常是[0，1]或[-1，1]。具体计算公式如下：

𝑥′=𝑥−𝑚𝑖𝑛(𝑥) / 𝑚𝑎𝑥(𝑥)−𝑚𝑖𝑛(𝑥)

其中， 𝑥′ 是归一化后的特征值， 𝑥 是原始特征值， 𝑚𝑖𝑛(𝑥) 和 𝑚𝑎𝑥(𝑥) 分别是特征的最小值和最大值。

Min-Max 归一化的优点是简单易懂，并且可以将数据限制在特定的范围内。它对于一些需要将数据映射到特定范围的算法或可视化方法（如神经网络）会非常合适。

示例代码：
```text
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# 示例数据
data = np.array([[1, 2], [3, 4], [5, 6]])
print("归一化前的数据：\n", data)

# 创建归一化对象，取值范围[0，1]
scaler = MinMaxScaler(feature_range=(0, 1))

# 对数据进行归一化
normalized_data = scaler.fit_transform(data)
print("归一化后的数据：\n", normalized_data)
```
输出结果
```text
归一化前的数据：
 [[1 2]
 [3 4]
 [5 6]]
归一化后的数据：
 [[0.  0. ]
 [0.5 0.5]
 [1.  1. ]]
```

#### **独热编码(One-Hot)编码**
独热编码(One-Hot)编码，又称为一位有效编码，是一种将分类变量转换为二进制向量的表示方法。这种方法主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由其独立的寄存器位表示，并且在任意时候只有一位有效。

One-Hot编码的过程如下：

-   首先，将分类值映射到整数值。例如，如果有三个工作类型，可以将它们映射为0、1和2。
-   然后，每个整数值被表示为二进制向量。对于上述的例子，三个工作类型可以表示为（1，0，0）、（0，1，0）和（0，0，1）。

One-Hot 编码适用于需要将分类变量转换为数值型变量的场景，例如在文本分类、推荐系统以及图像识别中，将文本中的词汇、用户的兴趣爱好以及图像的标签转换为向量表示。此外，在使用逻辑回归、决策树等机器学习算法时，One-Hot 编码可以应用于将离散特征的取值扩展到欧式空间，离散特征的某个取值就对应欧式空间的某个点，从而让特征之间的距离计算更加合理。

示例代码：

```text
import numpy as np

# 假设我们有以下类别标签的列表
categories = ['cat', 'dog', 'bird', 'cat', 'bird']
# 创建一个字典，将类别标签映射到整数值
category_to_int = {'cat': 0, 'dog': 1, 'bird': 2}

# 将类别标签转换为整数值
categories_int = [category_to_int[category] for category in categories]
# 获取唯一类别的数量
num_categories = len(category_to_int)

# 使用NumPy的onehot函数进行One-Hot编码
one_hot_encoded = np.eye(num_categories)[categories_int]

# 打印结果
print("原始类别数据:")
print(categories)
print("\n转换为整数值:")
print(categories_int)
print("\nOne-Hot编码:")
print(one_hot_encoded)
```

输出结果：

```text
原始类别数据:
['cat', 'dog', 'bird', 'cat', 'bird']

转换为整数值:
[0, 1, 2, 0, 2]

One-Hot编码:
[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]
 [1. 0. 0.]
 [0. 0. 1.]]
```
### TF-IDF 编码

TF-IDF（Term Frequency - Inverse Document Frequency）编码是一种常用的文本特征表示方法，用于衡量一个词在文档集合中的重要程度。

-   词频（TF）：表示某个词在所在文档中出现的频率。一般来说，一个词在文档中出现的次数越多，它就越重要。但是，为了避免长文档中的词被过度重视，通常会对词频进行标准化处理，即词频 = 某个词在文档中出现的次数 / 文档的总词数。
-   逆文档频率（IDF）：表示一个词在所有文档中的重要性。一个词如果在很多文档中都出现，那么它的IDF值就会较低；相反，如果一个词只在很少的文档中出现，那么它的IDF值就会较高。逆文档频率的计算公式为：IDF = log(总文档数 / 包含该词的文档数 + 1)。这里的加1是为了避免分母为零的情况。

最终，TF-IDF值是将词频和逆文档频率相乘得到的：TF-IDF = TF * IDF。这种计算方法倾向于过滤掉常见的词语，而保留重要的词语，因为常见的词语往往在所有文档中都出现，所以其IDF值较低，而重要的词语往往只在特定的文档中出现，所以其IDF值较高。

TF-IDF是一种非常有效的特征提取方法，广泛应用于文本分类、搜索引擎、推荐系统等领域。

示例代码：

```text
from sklearn.feature_extraction.text import TfidfVectorizer

# 假设我们有两篇文档
documents = [
    'This is the first document.',
    'This document is the second document.'
]

# 创建TfidfVectorizer对象
vectorizer = TfidfVectorizer()
# 使用TfidfVectorizer的fit_transform方法来计算TF-IDF矩阵
tfidf_matrix = vectorizer.fit_transform(documents)

# 打印出特征名称（即词汇表）
print("Feature names:", vectorizer.get_feature_names_out())

# 打印出TF-IDF矩阵
print("TF-IDF Matrix:")
print(tfidf_matrix.toarray())
```

输出结果：

```text
Feature names: ['document' 'first' 'is' 'second' 'the' 'this']
TF-IDF Matrix:
[[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901 ]
 [0.66758217 0.         0.33379109 0.46913173 0.33379109 0.33379109]]
```

#### **LabelEncoder 编码**

LabelEncoder是一种用于将类别型数据转化为数值型数据的编码技术。LabelEncoder按照类别的出现顺序，用0、1、2、3等整数值依次对每个不同的类别进行编码。例如，对于一个名为"颜色"的变量，其中包含红色、蓝色和绿色三个类别。LabelEncoder将红色编码为0，蓝色编码为1，绿色编码为2。

使用LabelEncoder时需要注意，虽然它可以将类别型数据转化为数值型数据，但它并未考虑类别之间的关系。也就是说，这种编码方式只是简单地将类别转化为数值，而并未考虑类别之间的顺序或语义关系。在某些情况下，这种顺序或语义关系可能会对模型的性能产生影响，因此在使用LabelEncoder时需要谨慎。

示例代码：

```text
from sklearn.preprocessing import LabelEncoder

# 假设我们有一组类别标签
labels = ['cat', 'dog', 'bird', 'cat', 'dog', 'bird']

# 初始化LabelEncoder
le = LabelEncoder()
# 使用fit方法来学习标签列表中的类别
le.fit(labels)
# 使用transform方法将标签转换为整数编码
encoded_labels = le.transform(labels)

# 打印出编码后的标签
print("Encoded labels:", encoded_labels)
# 如果你想将编码后的标签转换回原始标签，可以使用inverse_transform方法
original_labels = le.inverse_transform(encoded_labels)
# 打印出转换后的原始标签
print("Original labels:", original_labels)
```

输出结果：

```text
Encoded labels: [1 2 0 1 2 0]
Original labels: ['cat' 'dog' 'bird' 'cat' 'dog' 'bird']
```


### 数据增强

数据增强是指通过对原始数据进行一系列随机变换和处理，从而生成更多的数据，以增加数据的多样性和丰富性的技术。

举例说明：假设我们有一个图像分类任务，数据集包含各种动物的图片。如果训练数据仅包含正面拍摄的清晰图片，那么模型可能会过拟合这些特定条件下的数据，导致在模糊、旋转或遮挡等条件下的图片上表现不佳。


举例1：对图像进行旋转、平移、随即范围缩放、水平翻转操作

```text
import numpy as np
from keras.preprocessing.image import ImageDataGenerator

# 模拟数据的形状，例如一个批次包含32张128x128的RGB图像
batch_size = 32
img_width, img_height, img_channels = 128, 128, 3

# 创建模拟数据
# 这里我们创建了一个随机整数数组，模拟了图像像素值
x = np.random.randint(0, 256, (batch_size, img_width, img_height, img_channels), dtype=np.uint8)

# 创建ImageDataGenerator实例
# 这里我们定义了一些数据增强的操作
datagen = ImageDataGenerator(
    rotation_range=20,  # 随机旋转的角度范围
    width_shift_range=0.1,  # 随机水平平移的范围
    height_shift_range=0.1,  # 随机垂直平移的范围
    zoom_range=0.1,  # 随机缩放的范围
    horizontal_flip=True  # 随机水平翻转
)

# 使用ImageDataGenerator来生成增强的数据
# flow方法会生成一个无限的迭代器，它可以按需生成数据
augmented_images = datagen.flow(x, batch_size=batch_size)

# 从迭代器中获取一批增强的图像
# 注意，这里我们只是获取了一批模拟数据的增强版本，并没有实际进行训练
augmented_batch = next(augmented_images)

# 打印增强后的图像数组
print(augmented_batch)
```

举例2：对文本数据进行随机插入、删除

```text
import random

# 原始文本
text = "Hello world, this is a sample text."
# 增强文本
augmented_text = text

# 随机选择要插入的位置
insert_index = random.randint(0, len(text))
# 插入随机单词
random_word = random.choice([" Insert ", " Random ", " Word "])
augmented_text = augmented_text[:insert_index] + random_word + augmented_text[insert_index:]

# 随机选择要删除的位置
delete_index = random.randint(0, len(augmented_text) - 1)
augmented_text = augmented_text[:delete_index] + augmented_text[delete_index + 1:]

print("原始文本:", text)
print("增强文本:", augmented_text)
```

输出结果：

```text
原始文本: Hello world, this is a sample text.
增强文本: Hello Word  world, ths is a sample text.
```

#### 数据降维



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTgyNTE2MTE0LC0yMTEyMzQxNjg1LDk5ND
Y5OTY0MCwxMTU5NTMzMDgsMTk2NTE1NzEwOCwtOTAxMzM2MTMs
NDQ3OTgzNzU5LC00NjUxNzYzOTAsLTExNDYxMjQ0MjMsLTIwNT
gyMDQ5ODEsLTI2MDk1Mzg3Miw1MzgzNzc4MTBdfQ==
-->