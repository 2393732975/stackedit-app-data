
# 模型部署
### ONNX模型转换

让我们用下面的代码来把 PyTorch 的模型转换成 ONNX 格式的模型：

```python
x = torch.randn(1, 3, 256, 256) 
 
with torch.no_grad(): 
    torch.onnx.export( 
        model, 
        x, 
        "srcnn.onnx", 
        opset_version=11, 
        input_names=['input'], 
        output_names=['output'])
```
[模型部署入门教程（三）：PyTorch 转 ONNX 详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/498425043)

其中，**torch.onnx.export** 是 PyTorch 自带的把模型转换成 ONNX 格式的函数。让我们先看一下前三个必选参数：前三个参数分别是要转换的模型、模型的任意一组输入、导出的 ONNX 文件的文件名。转换模型时，需要原模型和输出文件名是很容易理解的，但为什么需要为模型提供一组输入呢？这就涉及到 ONNX 转换的原理了。从 PyTorch 的模型到 ONNX 的模型，本质上是一种语言上的翻译。直觉上的想法是像编译器一样彻底解析原模型的代码，记录所有控制流。但前面也讲到，我们通常只用 ONNX 记录不考虑控制流的静态图。因此，PyTorch 提供了一种叫做追踪（trace）的模型转换方法：给定一组输入，再实际执行一遍模型，即把这组输入对应的计算图记录下来，保存为 ONNX 格式。export 函数用的就是追踪导出方法，需要给任意一组输入，让模型跑起来。我们的测试图片是三通道，256x256大小的，这里也构造一个同样形状的随机张量。

剩下的参数中，opset_version 表示 ONNX 算子集的版本。深度学习的发展会不断诞生新算子，为了支持这些新增的算子，ONNX会经常发布新的算子集，目前已经更新15个版本。我们令 opset_version = 11，即使用第11个 ONNX 算子集，是因为 SRCNN 中的 bicubic （双三次插值）在 opset11 中才得到支持。剩下的两个参数 input_names, output_names 是输入、输出 tensor 的名称，我们稍后会用到这些名称。

如果上述代码运行成功，目录下会新增一个"srcnn.onnx"的 ONNX 模型文件。我们可以用下面的脚本来验证一下模型文件是否正确。

```python
import onnx 
 
onnx_model = onnx.load("srcnn.onnx") 
try: 
    onnx.checker.check_model(onnx_model) 
except Exception: 
    print("Model incorrect") 
else: 
    print("Model correct")
```

```
import torch.onnx 
# 转换的onnx格式的名称，文件后缀需为.onnx
onnx_file_name = "xxxxxx.onnx"
# 我们需要转换的模型，将torch_model设置为自己的模型
model = torch_model
# 加载权重，将model.pth转换为自己的模型权重
# 如果模型的权重是使用多卡训练出来，我们需要去除权重中多的module. 具体操作可以见5.4节
model = model.load_state_dict(torch.load("model.pth"))
# 导出模型前，必须调用model.eval()或者model.train(False)
model.eval()
# dummy_input就是一个输入的实例，仅提供输入shape、type等信息 
batch_size = 1 # 随机的取值，当设置dynamic_axes后影响不大
dummy_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True) 
# 这组输入对应的模型输出
output = model(dummy_input)
# 导出模型
torch.onnx.export(model,        # 模型的名称
                  dummy_input,   # 一组实例化输入
                  onnx_file_name,   # 文件保存路径/名称
                  export_params=True,        #  如果指定为True或默认, 参数也会被导出. 如果你要导出一个没训练过的就设为 False.
                  opset_version=10,          # ONNX 算子集的版本，当前已更新到15
                  do_constant_folding=True,  # 是否执行常量折叠优化
                  input_names = ['input'],   # 输入模型的张量的名称
                  output_names = ['output'], # 输出模型的张量的名称
                  # dynamic_axes将batch_size的维度指定为动态，
                  # 后续进行推理的数据可以与导出的dummy_input的batch_size不同
                  dynamic_axes={'input' : {0 : 'batch_size'},    
                                'output' : {0 : 'batch_size'}})
```

### TensorFlow Lite模型转换
#### 转换 SavedModel（推荐）

以下示例展示了如何将 [SavedModel](https://www.tensorflow.org/guide/saved_model?hl=zh-cn) 转换为 TensorFlow Lite 模型。
```
import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:  
	f.write(tflite_model)
```
#### 转换 Keras 模型

以下示例展示了如何将 [Keras](https://www.tensorflow.org/guide/keras/overview?hl=zh-cn) 模型转换为 TensorFlow Lite 模型。
```
import tensorflow as tf
# Create a model using high-level tf.keras.* APIs
model = tf.keras.models.Sequential([    
tf.keras.layers.Dense(units=1, input_shape=[1]),    
tf.keras.layers.Dense(units=16, activation='relu'),    
tf.keras.layers.Dense(units=1)
	])
model.compile(optimizer='sgd', loss='mean_squared_error') # compile the model
model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5) # train the model
# (to generate a SavedModel) tf.saved_model.save(model, "saved_model_keras_dir")
# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
# Save the model.
with open('model.tflite', 'wb') as f:  
	f.write(tflite_model)
```

#### 转换具体函数

以下示例展示了如何将[个具体函数](https://www.tensorflow.org/guide/intro_to_graphs?hl=zh-cn)转换为 TensorFlow Lite 模型。
![输入图片说明](/imgs/2024-05-14/CyYxnMgJwp7jY6Kn.png)

### Flask 应用
```
# 导入Flask类 
from flask import Flask 
# Flask类接收一个参数__name__ 
app = Flask(__name__) 
# 装饰器的作用是将路由映射到视图函数index 
@app.route('/') 
def index(): return 'Hello World' 
# Flask应用程序实例的run方法启动WEB服务器 
if __name__ == '__main__': 
	app.run()

```
https://juejin.cn/post/7205908717887373373
m
[推理模型部署(一)：ONNX runtime 实践 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/582974246)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyMDAxNjUyNDEsLTQ3OTYwOTgzNiwxOD
AzMjk2ODkwLDEzMzI4ODIxODgsLTE5MTc1MDIwOTcsNDczNjQy
NzUzLDIwNjQ3MTM2MjAsLTE5ODg3MjA4MDFdfQ==
-->