# 逻辑回归

逻辑回归（Logistic Regression），又叫逻辑斯蒂回归，是机器学习中一种十分基础的分类方法，由于算法简单而高效，在实际场景中得到了广泛的应用。本次实验中，我们将探索逻辑回归的原理及算法实现，并使用 scikit-learn 构建逻辑回归分类预测模型。
#### 知识点

-   线性可分和不可分
-   Sigmoid 分布函数
-   逻辑回归模型
-   对数损失函数
-   梯度下降法


线性可分和线性不可分
![输入图片说明](/imgs/2024-04-29/D2mhQ0UiafgCuBRJ.png)
关于0-1分类问题，线性回归解决的结果并不理想
### Sigmod 分布函数
![输入图片说明](/imgs/2024-04-29/WtcHb7rYG2RjqnvB.png)
```
def sigmoid(z): 
	sigmoid = 1 / (1 + np.exp(-z)) 
	return sigmoid
```
图像如下：
![输入图片说明](/imgs/2024-04-29/UZJISGkd2jWNzPJ7.png)
上图就是 Sigmoid 函数的图像，你会惊讶地发现，这个图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 1之间，且关于 𝑧=0 轴中心对称。同时当 𝑧 越大时，𝑦 越接近于 1，而 𝑧 越小时，𝑦 越接近于 0。如果我们以 0.5 为分界点，将 >0.5 或 <0.5 的值分为两类，这不就是解决 0−1 二分类问题的完美选择嘛。
### 逻辑回归模型
如果一组连续随机变量符合 Sigmoid 函数样本分布，就称作为逻辑分布。把线性函数拟合的结果使用 Sigmoid 函数压缩到 (0,1)之间。如果线性函数的 𝑦值越大，也就代表概率越接近于 1，反之接近于 0。
#### 对数损失函数
![输入图片说明](/imgs/2024-04-29/yms9QSUZc3XhbYcn.png)
```
def loss(h, y): 
	loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() 
	return loss
```


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NDcwNDQ5NjYsLTE1NDk3ODY5MiwtMT
U3NDQ1MzgxMyw0NDA5MDU2MTldfQ==
-->