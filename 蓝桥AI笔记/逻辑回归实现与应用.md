# 逻辑回归

逻辑回归（Logistic Regression），又叫逻辑斯蒂回归，是机器学习中一种十分基础的分类方法，由于算法简单而高效，在实际场景中得到了广泛的应用。本次实验中，我们将探索逻辑回归的原理及算法实现，并使用 scikit-learn 构建逻辑回归分类预测模型。
#### 知识点

-   线性可分和不可分
-   Sigmoid 分布函数
-   逻辑回归模型
-   对数损失函数
-   梯度下降法


线性可分和线性不可分
![输入图片说明](/imgs/2024-04-29/D2mhQ0UiafgCuBRJ.png)
关于0-1分类问题，线性回归解决的结果并不理想
### Sigmod 分布函数
![输入图片说明](/imgs/2024-04-29/WtcHb7rYG2RjqnvB.png)
```
def sigmoid(z): 
	sigmoid = 1 / (1 + np.exp(-z)) 
	return sigmoid
```
图像如下：
![输入图片说明](/imgs/2024-04-29/UZJISGkd2jWNzPJ7.png)
上图就是 Sigmoid 函数的图像，你会惊讶地发现，这个图像呈现出完美的 S 型（Sigmoid 的含义）。它的取值仅介于 0 和 1之间，且关于 𝑧=0 轴中心对称。同时当 𝑧 越大时，𝑦 越接近于 1，而 𝑧 越小时，𝑦 越接近于 0。如果我们以 0.5 为分界点，将 >0.5 或 <0.5 的值分为两类，这不就是解决 0−1 二分类问题的完美选择嘛。
### 逻辑回归模型
如果一组连续随机变量符合 Sigmoid 函数样本分布，就称作为逻辑分布。把线性函数拟合的结果使用 Sigmoid 函数压缩到 (0,1)之间。如果线性函数的 𝑦值越大，也就代表概率越接近于 1，反之接近于 0。
#### 对数损失函数
![输入图片说明](/imgs/2024-04-29/yms9QSUZc3XhbYcn.png)
```
def loss(h, y): 
	loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() 
	return loss
```
#### 梯度下降法
梯度是一个向量，梯度的方向指向函数值增加最快的方向，梯度的值表示函数值增加的速率。简而言之，对于一元函数而言，梯度就是指在某一点的导数。而对于多元函数而言，梯度就是指在某一点的偏导数组成的向量。

既然，函数在沿梯度方向变化最快，所以「梯度下降法」的核心就是，我们沿着梯度的反方向去寻找损失函数的极小值。过程如下图所示。
![输入图片说明](/imgs/2024-04-29/D2q6G3KHTq5WuhVF.png)

![输入图片说明](/imgs/2024-04-29/AQ1mRoaFy30DGmlf.png)
```
def gradient(X, h, y): 
	gradient = np.dot(X.T, (h - y)) / y.shape[0] 
	return gradient
```

### 逻辑回归的scikit-learn实现
在 scikit-learn 中，实现逻辑回归的类及默认参数是：
`LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)`
介绍其中几个常用的参数，其余使用默认即可：
-   `penalty`: 惩罚项，默认为 𝐿2 正则。
-   `dual`: 对偶化，默认为 False。
-   `tol`: 收敛阈值，当模型参数的更新量小于tol时，认为模型已经收敛，停止迭代（即使没有达到 `max_iter`），默认值为0.0001。
-   `fit_intercept`: 默认为 True，计算截距项。
-   `random_state`: 随机数发生器。
-   `max_iter`: 最大迭代次数，默认为 100。

另外，`solver` 参数用于指定求解损失函数的方法。默认为 `liblinear`（0.22 开始默认为 `lbfgs`），适合于小数据集。除此之外，还有适合多分类问题的 `newton-cg`, `sag`, `saga` 和 `lbfgs` 求解器。这些方法都来自于一些学术论文，有兴趣可以自行搜索了解。
```
from sklearn.linear_model import LogisticRegression

import matplotlib.pyplot as plt

import pandas as pd

import numpy as np

df = pd.read_csv(

"https://labfile.oss.aliyuncs.com/courses/1081/course-8-data.csv", header=0) # 加载数据集

x = df[['X0', 'X1']].values

y = df['Y'].values

model = LogisticRegression(tol=0.001, max_iter=10000, solver='liblinear')

model.fit(x, y)

plt.figure(figsize=(10, 6))#创建了一个新的图形窗口10*6英寸

plt.scatter(df['X0'], df['X1'], c=df['Y'])#画散点图，c为特征值，表示点的颜色

  

x1_min, x1_max = df['X0'].min(), df['X0'].max()

x2_min, x2_max = df['X1'].min(), df['X1'].max()

  

xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max),

np.linspace(x2_min, x2_max))

grid = np.c_[xx1.ravel(), xx2.ravel()]
#这行代码使用 `np.meshgrid()` 函数创建了一个二维网格，其中 `xx1` 对应于 x 轴上的点，`xx2` 对应于 y 轴上的点。`np.linspace()` 函数用于在最小值和最大值之间生成等间隔的点。
  

probs = (np.dot(grid, model.coef_.T) + model.intercept_).reshape(xx1.shape)
#  `grid = np.c_[xx1.ravel(), xx2.ravel()]`: 将二维网格展平为一维数组，并按列连接，形成一个矩阵。这样每个点的坐标就可以用一对 (x, y) 的形式表示。

plt.contour(xx1, xx2, probs, levels=[0], linewidths=1, colors='red')
#这行代码使用 `plt.contour()` 函数绘制等高线图，即决策边界。`xx1` 和 `xx2` 是网格点的坐标，`probs` 是对应的概率值。`levels=[0]` 表示只绘制概率为 0 的等高线，即决策边界。`linewidths=1` 设置等高线的宽度为 1，`colors='red'` 设置等高线的颜色为红色。

model.score(x, y)#模型在训练集上的分类准确率
```
![输入图片说明](/imgs/2024-04-29/2RCGvfXrSMQ9C8Ja.png)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTczMzY0NTI2MywtMTYyNjc5MDA2MiwtMT
U0OTc4NjkyLC0xNTc0NDUzODEzLDQ0MDkwNTYxOV19
-->