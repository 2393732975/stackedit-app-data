# 决策树和K近邻

### 决策树
熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。（大多数学过物理化学的人应该很熟悉这个概念，没什么变化。）

信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。（说白了也是讲信息的混乱程度，我的理解是：一堆信息中不同的类很多，说明混乱程度高，熵就高；一堆信息中只有一两种类，那熵就比较低。）


信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。（就是划分数据到不同子集前后，熵的变化。）

#### sklearn.tree.DecisionTreeClassifier 类
sklearn.tree.DecisionTreeClassifier 类的主要参数为：
-   max_depth 树的最大深度；
-   max_features 搜索最佳分区时的最大特征数（特征很多时，设置这个参数很有必要，因为基于所有特征搜索分区会很「昂贵」）；
-   min_samples_leaf 叶节点的最少样本数。
```
reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17) 
reg_tree.fit(X_train, y_train) 
reg_tree_pred = reg_tree.predict(X_test)
```
### K近邻
最近邻方法（K 近邻或 k-NN）是另一个非常流行的分类方法。当然，也可以用于回归问题。和决策树类似，这是最容易理解的分类方法之一。这一方法遵循紧密性假说：如果样本间的距离能以足够好的方法衡量，那么相似的样本更可能属于同一分类。

k-NN 分类/回归的效果取决于一些参数：

-   邻居数 k。
-   样本之间的距离度量（常见的包括 Hamming，欧几里得，余弦和 Minkowski 距离）。注意，大部分距离要求数据在同一尺度下，例如「薪水」特征的数值在千级，「年龄」特征的数值却在百级，如果直接将他们丢进最近邻模型中，「年龄」特征就会受到比较大的影响。
-   邻居的权重（每个邻居可能贡献不同的权重，例如，样本越远，权重越低）。

#### scikit-learn 的 KNeighborsClassifier 类

`sklearn.neighbors.KNeighborsClassifier` 类的主要参数为：

-   weights：可设为 uniform（所有权重相等），distance（权重和到测试样本的距离成反比），或任何其他用户自定义的函数。
-   algorithm（可选）：可设为 brute、ball_tree、KD_tree、auto。若设为 brute，通过训练集上的网格搜索来计算每个测试样本的最近邻；若设为 ball_tree 或 KD_tree，样本间的距离储存在树中，以加速寻找最近邻；若设为 auto，将基于训练集自动选择合适的寻找最近邻的方法。
-   leaf_size（可选）：若寻找最近邻的算法是 BallTree 或 KDTree，则切换为网格搜索所用的阈值。
-   metric：可设为 minkowski、manhattan、euclidean、chebyshev 或其他。


### 验证模型的质量
-   留置法。保留一小部分数据（一般是 20% 到 40%）作为留置集，在其余数据上训练模型（原数据集的 60%-80%），然后在留置集上验证模型的质量。
-   交叉验证。最常见的情形是 k 折交叉验证，如下图所示。
![输入图片说明](/imgs/2024-04-30/0AH8fYoa5r1uiQtZ.png)
在 k 折交叉验证中，模型在原数据集的 𝐾−1K−1 个子集上进行训练（上图白色部分），然后在剩下的 1 个子集上验证表现，重复训练和验证的过程，每次使用不同的子集（上图橙色部分），总共进行 K 次，由此得到 K 个模型质量评估指数，通常用这些评估指数的求和平均数来衡量分类/回归模型的总体质量。

相比留置法，交叉验证能更好地评估模型在新数据上的表现。然而，当你有大量数据时，交叉验证对机器计算能力的要求会变得很高。

### 比较在决策树和最近邻方法（通过实例）
```
import pandas as pd 
df = pd.read_csv( 'https://labfile.oss.aliyuncs.com/courses/1283/telecom_churn.csv') 
df['International plan'] = pd.factorize(df['International plan'])[0] 
df['Voice mail plan'] = pd.factorize(df['Voice mail plan'])[0] 
df['Churn'] = df['Churn'].astype('int') 
states = df['State'] 
y = df['Churn'] 
df.drop(['State', 'Churn'], axis=1, inplace=True)
```
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTkzNDcxNzcwMywtMTY3MTg0ODQ0MCwtMT
cxMjU0MDQ2NywxNDUwOTE0ODEwLC05NTYyMDIxNCwtMTkzNjY2
NDgwOCw1NTkyODU1NzZdfQ==
-->