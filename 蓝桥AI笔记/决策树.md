# 决策树和K近邻

### 决策树
熵（entropy）： 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。（大多数学过物理化学的人应该很熟悉这个概念，没什么变化。）

信息论（information theory）中的熵（香农熵）： 是一种信息的度量方式，表示信息的混乱程度，也就是说：信息越有序，信息熵越低。（说白了也是讲信息的混乱程度，我的理解是：一堆信息中不同的类很多，说明混乱程度高，熵就高；一堆信息中只有一两种类，那熵就比较低。）


信息增益（information gain）： 在划分数据集前后信息发生的变化称为信息增益。（就是划分数据到不同子集前后，熵的变化。）

#### sklearn.tree.DecisionTreeClassifier 类
sklearn.tree.DecisionTreeClassifier 类的主要参数为：
-   max_depth 树的最大深度；
-   max_features 搜索最佳分区时的最大特征数（特征很多时，设置这个参数很有必要，因为基于所有特征搜索分区会很「昂贵」）；
-   min_samples_leaf 叶节点的最少样本数。
```
reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17) 
reg_tree.fit(X_train, y_train) 
reg_tree_pred = reg_tree.predict(X_test)
```
### K近邻
最近邻方法（K 近邻或 k-NN）是另一个非常流行的分类方法。当然，也可以用于回归问题。和决策树类似，这是最容易理解的分类方法之一。这一方法遵循紧密性假说：如果样本间的距离能以足够好的方法衡量，那么相似的样本更可能属于同一分类。

k-NN 分类/回归的效果取决于一些参数：

-   邻居数 k。
-   样本之间的距离度量（常见的包括 Hamming，欧几里得，余弦和 Minkowski 距离）。注意，大部分距离要求数据在同一尺度下，例如「薪水」特征的数值在千级，「年龄」特征的数值却在百级，如果直接将他们丢进最近邻模型中，「年龄」特征就会受到比较大的影响。
-   邻居的权重（每个邻居可能贡献不同的权重，例如，样本越远，权重越低）。

#### scikit-learn 的 KNeighborsClassifier 类

`sklearn.neighbors.KNeighborsClassifier` 类的主要参数为：

-   weights：可设为 uniform（所有权重相等），distance（权重和到测试样本的距离成反比），或任何其他用户自定义的函数。
-   algorithm（可选）：可设为 brute、ball_tree、KD_tree、auto。若设为 brute，通过训练集上的网格搜索来计算每个测试样本的最近邻；若设为 ball_tree 或 KD_tree，样本间的距离储存在树中，以加速寻找最近邻；若设为 auto，将基于训练集自动选择合适的寻找最近邻的方法。
-   leaf_size（可选）：若寻找最近邻的算法是 BallTree 或 KDTree，则切换为网格搜索所用的阈值。
-   metric：可设为 minkowski、manhattan、euclidean、chebyshev 或其他。


### 验证模型的质量

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTQ1MDkxNDgxMCwtOTU2MjAyMTQsLTE5Mz
Y2NjQ4MDgsNTU5Mjg1NTc2XX0=
-->