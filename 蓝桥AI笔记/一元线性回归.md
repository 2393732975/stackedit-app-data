
# 一元线性回归

### 平方损失函数
![输入图片说明](/imgs/2024-04-29/KizZucoo5Ovmyg0j.png)
```
def square_loss(x, y, w0, w1): 
	loss = sum(np.square(y - (w0 + w1*x))) 
	return loss
```
### 最小二乘法
通过对平方损失函数的变量w0和w1分别求偏导，然后置零求得如下：
![输入图片说明](/imgs/2024-04-29/yyxo6pvX4hUGywqF.png)
```
def w_calculator(x, y): 
	n = len(x) 
	w1 = (n*sum(x*y) - sum(x)*sum(y))/(n*sum(x*x) - sum(x)*sum(x)) 
	w0 = (sum(x*x)*sum(y) - sum(x)*sum(x*y))/(n*sum(x*x)-sum(x)*sum(x)) 
	return w0, w1
```
### 线性回归 scikit-learn 实现

```
from sklearn.linear_model import LinearRegression 

# 定义线性回归模型 
model = LinearRegression() 
model.fit(x.reshape(len(x), 1), y) # 训练, reshape 操作把数据处理成 fit 能接受的形状 

# 得到模型拟合参数 
model.intercept_, model.coef_
```
`LinearRegression()` 类
`sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)`

- fit_intercept: 默认为 True，计算截距项。
- normalize: 默认为 False，不针对数据进行标准化处理。
- copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。
- n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。

### 最小二乘法的矩阵推导及实现
![输入图片说明](/imgs/2024-04-29/1w1kFMMMPmSl5PgQ.png)
![输入图片说明](/imgs/2024-04-29/K0OTcRSS6XlSfBof.png)
```
def w_matrix(x, y): 
	w = (x.T * x).I * x.T * y 
	return w
```
### 线性回归预测实战

![输入图片说明](/imgs/2024-04-29/NlBlrVfcjGifnKnk.png)![](https://doc.shiyanlou.com/document-uid214893labid6102timestamp1531366212104.png)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTYwNzcwNzAzNywtMjAyNzM2MTIwMCwtMT
M1NjE3MTIwNl19
-->